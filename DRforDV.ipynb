{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "This project experimented 3 interesting, but rather complex in terms of implementation methods for dimensionality reduction. These are:\n",
    "1. Principle Component Analysis (PCA)\n",
    "2. Factor Analysis (FA)\n",
    "3. Random Forest (RF)\n",
    "\n",
    "Among 3, only PCA is able to reduce the dimension through a way that combine multiple variables using algorithm. Both the FA and RF works in identifying important variables and provide statistical evidence on the removal of useless variables for details, please read Literature Review, The coding comes after Literature Review Section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Literature Review\n",
    "There are more than a few way of implementing dimensionality reduction. Which some are simple to understand but only useful in meaninglessly few way.\n",
    "1. <strong>Missing Values Ratio</strong>: Remove variables with high missing values   systematically\n",
    "\n",
    "2. <strong>Low Variance Filter</strong>: Remove variables with low variances\n",
    "\n",
    "3. <strong>High Correlation Filter</strong>: Remove inter-related variables\n",
    "\n",
    "The application of these methods could be easily implemented through the use of df.isna(), Normalizer(), and corr() and therefore will not be focused in this project. Instead, there are a few more complex techniques:\n",
    "\n",
    "4. <strong>Random Forest/Ensemble Trees</strong>: Use Random Forest to recognize statistically powerful columns.\n",
    "\n",
    "5. <strong>Principal Component Analysis(PCA)</strong>: Variance based method, combine multiple dimensional data into a single or a few variables through calculations. Using simple variableA*VariableB... as a baseline, PCA is known to perform better in information conservation through its calculations.\n",
    "\n",
    "6. <strong>Backward Feature Elimination</strong>: Take all variable, train the dataset. Remove 1 each run to acquire the smallest number of variables required for the model to perform.\n",
    "\n",
    "7. <strong>Forward Feature Construction</strong>: Inverse to (6), take 1 column, increase one each time.\n",
    "\n",
    "8. <strong>Factor Analysis</strong>: Identifying the variables that are powerful, similar to 6,7 but different algorithm, in this case, relies on the euclidean distances instead of trial and error as in (6) and (7).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
